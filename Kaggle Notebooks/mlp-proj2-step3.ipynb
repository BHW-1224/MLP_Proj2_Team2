{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":3992867,"sourceType":"datasetVersion","datasetId":2369168},{"sourceId":6570909,"sourceType":"datasetVersion","datasetId":3795728},{"sourceId":629613,"sourceType":"modelInstanceVersion","modelInstanceId":474328,"modelId":490210},{"sourceId":629671,"sourceType":"modelInstanceVersion","modelInstanceId":474381,"modelId":490265},{"sourceId":630248,"sourceType":"modelInstanceVersion","modelInstanceId":474881,"modelId":490777},{"sourceId":631595,"sourceType":"modelInstanceVersion","modelInstanceId":476031,"modelId":491945},{"sourceId":631721,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476132,"modelId":492051}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import PeftModel, PeftConfig # â­ï¸ [ìˆ˜ì •] PeftConfig ì„í¬íŠ¸\nfrom sklearn.preprocessing import StandardScaler\nfrom safetensors.torch import load_file\n\nprint(\"--- [1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ ---\")\n\n\n\nBASE_MODEL_PATH = \"/kaggle/input/debertav3large\"\nSAVED_MODEL_PATH = \"/kaggle/input/real-jonna-last/transformers/default/1\" \nTEST_CSV_PATH = \"/kaggle/input/llm-classification-finetuning/test.csv\"\nVADER_PATH = \"/kaggle/input/vader/transformers/default/1/vader_model.pkl\"\nMAX_LEN = 512\nBATCH_SIZE = 16 \nBIAS_DIM = 6\n\n\ndef style_counts(text):\n    text = str(text) \n    ex = text.count(\"!\")\n    qm = text.count(\"?\")\n    co = text.count(\",\")\n    return ex, qm, co\n\nclass HybridClassifier(nn.Module):\n    def __init__(self, backbone, bias_dim, num_labels=3):\n        super().__init__()\n        self.backbone = backbone \n        classifier_in_features = self.backbone.classifier.in_features \n        self.bias_proj = nn.Sequential(\n            nn.Linear(bias_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1)\n        )\n        self.classifier = nn.Linear(classifier_in_features + 64, num_labels)\n        self.backbone.classifier = nn.Identity() \n\n    def forward(self, input_ids=None, attention_mask=None, bias_features=None, labels=None):\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n        cls_emb = outputs.logits \n        bias_vec = self.bias_proj(bias_features)\n        fused = torch.cat([cls_emb, bias_vec], dim=1) \n        logits = self.classifier(fused) \n        loss = None\n        if labels is not None:\n            loss = nn.CrossEntropyLoss()(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}\n\nprint(\"--- [2] í›ˆë ¨ìš© í´ë˜ìŠ¤/í•¨ìˆ˜ ì¬ì •ì˜ ì™„ë£Œ ---\")\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (ğŸ¥‡ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œì—ì„œ)\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH)\n    print(f\"'{SAVED_MODEL_PATH}'ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.\")\nexcept OSError:\n    print(f\"ì˜¤ë¥˜: '{SAVED_MODEL_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Input ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    exit()\n\n# 2. VADER ë¡œë“œ (ğŸ… VADER ê²½ë¡œì—ì„œ)\ntry:\n    sia = joblib.load(VADER_PATH)\n    print(f\"'{VADER_PATH}' ë¡œë“œ ì™„ë£Œ.\")\nexcept FileNotFoundError:\n    print(f\"ì˜¤ë¥˜: '{VADER_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Input ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    exit()\n\n# 3. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (ğŸ¥‡ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œì—ì„œ)\ntry:\n    scaler_path = os.path.join(SAVED_MODEL_PATH, \"scaler.joblib\")\n    scaler = joblib.load(scaler_path)\n    print(f\"'{scaler_path}'ì—ì„œ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ.\")\nexcept FileNotFoundError:\n    print(f\"ì˜¤ë¥˜: '{scaler_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Input ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    exit()\n\n# 4. ëª¨ë¸ ë¡œë“œ (PEFT + ì»¤ìŠ¤í…€ í—¤ë“œ)\nprint(\"--- [3] ëª¨ë¸ ë¡œë“œ ì‹œì‘ ---\")\n# 4-1. ì›ë³¸(Base) DeBERTa ë¼ˆëŒ€ ë¡œë“œ (ğŸ¥ˆ ì›ë³¸ ëª¨ë¸ ê²½ë¡œì—ì„œ)\nbase_backbone = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_PATH, num_labels=3)\nprint(\"  (a) ì›ë³¸ DeBERTa ë¼ˆëŒ€ ë¡œë“œ ì™„ë£Œ.\")\n\n# 4-2. ë¼ˆëŒ€ë¥¼ HybridClassifier ì•„í‚¤í…ì²˜ë¡œ ê°ì‹¸ê¸°\nbase_hybrid_model = HybridClassifier(base_backbone, bias_dim=BIAS_DIM)\nprint(\"  (b) HybridClassifier ì•„í‚¤í…ì²˜ êµ¬ì„± ì™„ë£Œ.\")\n\n# 4-3. ì»¤ìŠ¤í…€ í—¤ë“œ(model.safetensors) ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ\nhead_weights_path = os.path.join(SAVED_MODEL_PATH, \"model.safetensors\")\ntry:\n    head_weights = load_file(head_weights_path)\n    base_hybrid_model.load_state_dict(head_weights, strict=False)\n    print(\"  (c) ì»¤ìŠ¤í…€ í—¤ë“œ(model.safetensors) ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ ì™„ë£Œ.\")\nexcept FileNotFoundError:\n    print(f\"ì˜¤ë¥˜: '{head_weights_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Input ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    exit()\nexcept Exception as e:\n    print(f\"ì˜¤ë¥˜: ì»¤ìŠ¤í…€ í—¤ë“œ ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\")\n    exit()\n\n\nprint(\"  (d) LoRA ì–´ëŒ‘í„° ì„¤ì •(config) ë¡œë“œ ì¤‘...\")\ntry:\n    config = PeftConfig.from_pretrained(SAVED_MODEL_PATH)\nexcept Exception as e:\n    print(f\"ì˜¤ë¥˜: '{SAVED_MODEL_PATH}'ì—ì„œ adapter_config.json ë¡œë“œ ì‹¤íŒ¨: {e}\")\n    exit()\n\n\nmodel = PeftModel(base_hybrid_model, config)\nprint(\"  (e) PeftModel ì•„í‚¤í…ì²˜ êµ¬ì„± ì™„ë£Œ.\")\n\n\nadapter_weights_path = os.path.join(SAVED_MODEL_PATH, \"adapter_model.safetensors\")\ntry:\n    adapter_weights = load_file(adapter_weights_path)\n    # strict=False\n    model.load_state_dict(adapter_weights, strict=False) \n    print(\"  (f) LoRA ì–´ëŒ‘í„°(adapter_model.safetensors) ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ ì™„ë£Œ.\")\nexcept FileNotFoundError:\n    print(f\"ì˜¤ë¥˜: '{adapter_weights_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n    exit()\nexcept Exception as e:\n    print(f\"ì˜¤ë¥˜: ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\")\n    exit()\n\n\nmodel.to(device)\nmodel.eval() \nprint(f\"--- [4] ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({device}) ---\")\n\n\n\nprint(\"--- [5] í…ŒìŠ¤íŠ¸ ë°ì´í„°(test.csv) ì¶”ë¡  ì‹œì‘ ---\")\ntry:\n    test = pd.read_csv(TEST_CSV_PATH)\n    print(\"test.csv ë¡œë“œ ì™„ë£Œ.\")\nexcept FileNotFoundError:\n    print(f\"ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Input ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n    exit()\n\n# 1. í›ˆë ¨ ë•Œì™€ ë™ì¼í•˜ê²Œ í”¼ì²˜ ìƒì„±\ntest[\"a_sentiment\"] = test[\"response_a\"].apply(lambda x: sia.polarity_scores(str(x))[\"compound\"])\ntest[\"b_sentiment\"] = test[\"response_b\"].apply(lambda x: sia.polarity_scores(str(x))[\"compound\"])\ntest[\"a_len\"] = test[\"response_a\"].astype(str).apply(len)\ntest[\"b_len\"] = test[\"response_b\"].astype(str).apply(len)\ntest[\"a_word_cnt\"] = test[\"response_a\"].astype(str).apply(lambda x: len(x.split()))\ntest[\"b_word_cnt\"] = test[\"response_b\"].astype(str).apply(lambda x: len(x.split()))\n\na_style = test[\"response_a\"].apply(style_counts).apply(pd.Series)\nb_style = test[\"response_b\"].apply(style_counts).apply(pd.Series)\na_style.columns = [\"a_exclam\", \"a_qmark\", \"a_commas\"]\nb_style.columns = [\"b_exclam\", \"b_qmark\", \"b_commas\"]\ntest = pd.concat([test, a_style, b_style], axis=1)\n\nfor col in [\"len\", \"word_cnt\", \"sentiment\", \"exclam\", \"qmark\", \"commas\"]:\n    test[f\"diff_{col}\"] = test[f\"a_{col}\"] - test[f\"b_{col}\"]\n\nbias_feats_list = [\"diff_len\", \"diff_word_cnt\", \"diff_sentiment\", \"diff_exclam\", \"diff_qmark\", \"diff_commas\"]\ntest_bias_unscaled = test[bias_feats_list].values\n\n# 2. ë¡œë“œí•œ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ 'transform'\ntest_bias_scaled = scaler.transform(test_bias_unscaled)\ntest_bias_tensor = torch.tensor(test_bias_scaled, dtype=torch.float).to(device)\nprint(\"í…ŒìŠ¤íŠ¸ í”¼ì²˜ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ.\")\n\n# 3. í…ìŠ¤íŠ¸ í”¼ì²˜ ìƒì„±\nsep = tokenizer.sep_token or \"</s>\"\ntest_texts = (test[\"prompt\"] + f\" {sep} \" +\n              test[\"response_a\"] + f\" {sep} \" +\n              test[\"response_b\"]).tolist()\n\n# 4. í† í¬ë‚˜ì´ì§•\nencodings = tokenizer(\n    test_texts, \n    padding=True, \n    truncation=True, \n    max_length=MAX_LEN, \n    return_tensors=\"pt\"\n)\nencodings = {k: v.to(device) for k, v in encodings.items()}\n\n# 5. ë°°ì¹˜ ì¶”ë¡ \nall_preds = []\nprint(\"ë°°ì¹˜ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\nfor i in range(0, len(test_texts), BATCH_SIZE):\n    batch_input_ids = encodings[\"input_ids\"][i:i+BATCH_SIZE]\n    batch_attention_mask = encodings[\"attention_mask\"][i:i+BATCH_SIZE]\n    batch_bias_features = test_bias_tensor[i:i+BATCH_SIZE]\n    \n    with torch.no_grad():\n        outputs = model(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask,\n            bias_features=batch_bias_features\n        )\n        \n        logits = outputs[\"logits\"]\n        preds = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n        all_preds.append(preds)\n\nall_preds = np.concatenate(all_preds, axis=0)\nprint(\"ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ.\")\n\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"winner_model_a\": all_preds[:, 0],\n    \"winner_model_b\": all_preds[:, 1],\n    \"winner_tie\": all_preds[:, 2],\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"--- [6] âœ… submission.csv íŒŒì¼ ìƒì„± ì™„ë£Œ ---\")\n\n# --- ë©”ëª¨ë¦¬ ì •ë¦¬ ---\ndel model, base_backbone, base_hybrid_model, tokenizer, test_bias_tensor, encodings, test, scaler, sia\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\"\\n--- [7] ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T10:24:14.986048Z","iopub.execute_input":"2025-11-06T10:24:14.986251Z","iopub.status.idle":"2025-11-06T10:24:46.087704Z","shell.execute_reply.started":"2025-11-06T10:24:14.986233Z","shell.execute_reply":"2025-11-06T10:24:46.087051Z"}},"outputs":[{"name":"stderr","text":"2025-11-06 10:24:28.817891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762424668.988172      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762424669.039269      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--- [1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ ---\n--- [2] í›ˆë ¨ìš© í´ë˜ìŠ¤/í•¨ìˆ˜ ì¬ì •ì˜ ì™„ë£Œ ---\n'/kaggle/input/real-jonna-last/transformers/default/1'ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.\n'/kaggle/input/vader/transformers/default/1/vader_model.pkl' ë¡œë“œ ì™„ë£Œ.\n'/kaggle/input/real-jonna-last/transformers/default/1/scaler.joblib'ì—ì„œ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ.\n--- [3] ëª¨ë¸ ë¡œë“œ ì‹œì‘ ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/debertav3small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  (a) ì›ë³¸ DeBERTa ë¼ˆëŒ€ ë¡œë“œ ì™„ë£Œ.\n  (b) HybridClassifier ì•„í‚¤í…ì²˜ êµ¬ì„± ì™„ë£Œ.\n  (c) ì»¤ìŠ¤í…€ í—¤ë“œ(model.safetensors) ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ ì™„ë£Œ.\n  (d) LoRA ì–´ëŒ‘í„° ì„¤ì •(config) ë¡œë“œ ì¤‘...\n  (e) PeftModel ì•„í‚¤í…ì²˜ êµ¬ì„± ì™„ë£Œ.\n  (f) LoRA ì–´ëŒ‘í„°(adapter_model.safetensors) ê°€ì¤‘ì¹˜ ìˆ˜ë™ ë¡œë“œ ì™„ë£Œ.\n--- [4] ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (cuda) ---\n--- [5] í…ŒìŠ¤íŠ¸ ë°ì´í„°(test.csv) ì¶”ë¡  ì‹œì‘ ---\ntest.csv ë¡œë“œ ì™„ë£Œ.\ní…ŒìŠ¤íŠ¸ í”¼ì²˜ ìƒì„± ë° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ.\në°°ì¹˜ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...\në°°ì¹˜ ì¶”ë¡  ì™„ë£Œ.\n--- [6] âœ… submission.csv íŒŒì¼ ìƒì„± ì™„ë£Œ ---\n\n--- [7] ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ---\n","output_type":"stream"}],"execution_count":1}]}